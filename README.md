MercadoPago ML Pipeline

Goal: build a machine learningâ€“ready dataset for ordering Value Propositions (a.k.a. Value Props) shown in MercadoPagoâ€™s â€œDescubrÃ­ MÃ¡sâ€ carousel.

The pipeline ingests prints, taps, and payments (JSONL/CSV), validates them with Pydantic (row-level) and Pandera (DataFrame-level), applies quality checks, and emits datasets ready for feature engineering.

ğŸ“‚ Repository Structure

mercadopago-ml-pipeline/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ params.yaml             # Central configuration (paths, windows, thresholds)
â”œâ”€â”€ management/
â”‚   â””â”€â”€ logging_config.py       # Logging bootstrap & ProgressTracker
â”œâ”€â”€ src/
â”‚   â””â”€â”€ carrusel_mp/
â”‚       â””â”€â”€ config.py           # Config loader, env overrides, validation bounds
â”‚
â”‚   â””â”€â”€ data/
â”‚       â”œâ”€â”€ extractors.py       # DataExtractor base + Prints/Taps/Payments
â”‚       â”œâ”€â”€ loaders.py          # JSONL/CSV chunked readers
â”‚       â””â”€â”€ validators.py       # Cross-source validators (e.g. taps must have prints)
â”‚
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ contracts.py        # Pandera schemas (raw, processed, final)
â”‚       â””â”€â”€ schemas.py          # Pydantic models (Print, Tap, Payment, Results)
â”‚
â””â”€â”€ data/                       # Storage (gitignored in real usage)
    â”œâ”€â”€ raw/                    # Bronze: raw inputs
    â”œâ”€â”€ processed/              # Silver: cleaned with metadata
    â””â”€â”€ output/                 # Gold: ML-ready dataset

ğŸ”‘ Core Components
1. Configuration (config/params.yaml + src/carrusel_mp/config.py)

params.yaml defines paths, chunk sizes, memory caps, validation windows, thresholds.

Config dataclass loader:

Reads YAML and applies environment variable overrides.

Provides validation bounds (e.g., timestamp age in days).

Ensures required directories exist.

2. Logging (management/logging_config.py)

Uses logging.config.dictConfig for structured logs.

File rotation with size/backup limits.

Console logging toggle.

ProgressTracker for stepwise reporting:
 
 ``` yaml
ğŸš€ start: extraction
ğŸ“ˆ [2/5] (40.0%) processed taps
âœ… done: extraction (duration: 12.34s)
```

3. Models

Pydantic Schemas (schemas.py)

Validate each row (PrintRecord, TapRecord, PaymentRecord).

Enforce ID formats, timestamp bounds (UTC), amount limits.

Pandera Contracts (contracts.py)

Validate entire DataFrames.

strict=False for raw (allow optional cols like position).

strict=True for processed/final (lock schema).

Capture business rules (e.g., taps â‰¤ prints, payments â‰¤ max).

4. Data Extractors (extractors.py)

Abstract DataExtractor:

Generic over Pydantic model + Pandera schema.

Reads chunked data (via loaders.py).

Normalizes raw JSON/CSV into canonical form:

timestamp (UTC, parsed from day / pay_date).

user_id, value_prop_id.

position (prints/taps) or amount (payments).

Rejects invalid rows â†’ quarantine CSV.

Writes manifest JSON with metrics (rows read, rejected, etc.).

Concrete extractors:

PrintsExtractor â†’ JSONL with event_data.

TapsExtractor â†’ JSONL with event_data.

PaymentsExtractor â†’ CSV with pay_date, total.

5. Validators (validators.py)

Example: taps_have_prints
Ensures each tap has a corresponding print (same user_id, value_prop_id, date).

ğŸ§© Data Flow
ETL Flow (Bronze â†’ Silver â†’ Gold)

``` text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  prints.jsonl â”‚â”€â”€â”€â–º â”‚ PrintsExtractor  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   taps.jsonl  â”‚â”€â”€â”€â–º â”‚ TapsExtractor    â”‚â”€â”€â”€â–º â”‚ Feature Engine /        â”‚â”€â”€â”€â–º Final dataset
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”‚ Aggregator (TBD)        â”‚
â”‚   pays.csv    â”‚â”€â”€â”€â–º â”‚ PaymentsExtractorâ”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

Extraction Workflow

``` text[Raw file]
   â”‚
   â–¼
[Loader (chunk)]
   â”‚
   â–¼
[Normalize]
   â”‚
   â–¼
[Pydantic row validate]
   â”‚
   â–¼
[Pandera DF validate]
   â”‚
   â–¼
[Processed chunk] â†’ [Yield / Write]

```

âš™ï¸ Usage (so far)
Configure

Edit config/params.yaml:

``` yaml
processing:
  chunk_size: 10000
  target_window_days: 7
  lookback_window_days: 21
validation:
  business_rules:
    min_timestamp_days_ago: 1825

```
Run Extraction

Example (inside scripts/ or notebook):

``` python
from src.data.extractors import PrintsExtractor
from carrusel_mp.config import get_config

cfg = get_config()
prints_path = cfg.data_paths.raw + "/prints.json"

ext = PrintsExtractor(prints_path)
for chunk in ext.extract_chunked():
    print(chunk.head())

```


