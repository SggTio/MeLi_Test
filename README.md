MercadoPago ML Pipeline

Goal: build a machine learning‚Äìready dataset for ordering Value Propositions (a.k.a. Value Props) shown in MercadoPago‚Äôs ‚ÄúDescubr√≠ M√°s‚Äù carousel.

The pipeline ingests prints, taps, and payments (JSONL/CSV), validates them with Pydantic (row-level) and Pandera (DataFrame-level), applies quality checks, and emits datasets ready for feature engineering.

üìÇ Repository Structure

mercadopago-ml-pipeline/
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ params.yaml             # Central configuration (paths, windows, thresholds)
‚îú‚îÄ‚îÄ management/
‚îÇ   ‚îî‚îÄ‚îÄ logging_config.py       # Logging bootstrap & ProgressTracker
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ carrusel_mp/
‚îÇ       ‚îî‚îÄ‚îÄ config.py           # Config loader, env overrides, validation bounds
‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ data/
‚îÇ       ‚îú‚îÄ‚îÄ extractors.py       # DataExtractor base + Prints/Taps/Payments
‚îÇ       ‚îú‚îÄ‚îÄ loaders.py          # JSONL/CSV chunked readers
‚îÇ       ‚îî‚îÄ‚îÄ validators.py       # Cross-source validators (e.g. taps must have prints)
‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ models/
‚îÇ       ‚îú‚îÄ‚îÄ contracts.py        # Pandera schemas (raw, processed, final)
‚îÇ       ‚îî‚îÄ‚îÄ schemas.py          # Pydantic models (Print, Tap, Payment, Results)
‚îÇ
‚îî‚îÄ‚îÄ data/                       # Storage (gitignored in real usage)
    ‚îú‚îÄ‚îÄ raw/                    # Bronze: raw inputs
    ‚îú‚îÄ‚îÄ processed/              # Silver: cleaned with metadata
    ‚îî‚îÄ‚îÄ output/                 # Gold: ML-ready dataset

üîë Core Components
1. Configuration (config/params.yaml + src/carrusel_mp/config.py)

params.yaml defines paths, chunk sizes, memory caps, validation windows, thresholds.

Config dataclass loader:

Reads YAML and applies environment variable overrides.

Provides validation bounds (e.g., timestamp age in days).

Ensures required directories exist.

2. Logging (management/logging_config.py)

Uses logging.config.dictConfig for structured logs.

File rotation with size/backup limits.

Console logging toggle.

ProgressTracker for stepwise reporting:
 
 ``` yaml
üöÄ start: extraction
üìà [2/5] (40.0%) processed taps
‚úÖ done: extraction (duration: 12.34s)
```

3. Models

Pydantic Schemas (schemas.py)

Validate each row (PrintRecord, TapRecord, PaymentRecord).

Enforce ID formats, timestamp bounds (UTC), amount limits.

Pandera Contracts (contracts.py)

Validate entire DataFrames.

strict=False for raw (allow optional cols like position).

strict=True for processed/final (lock schema).

Capture business rules (e.g., taps ‚â§ prints, payments ‚â§ max).

4. Data Extractors (extractors.py)

Abstract DataExtractor:

Generic over Pydantic model + Pandera schema.

Reads chunked data (via loaders.py).

Normalizes raw JSON/CSV into canonical form:

timestamp (UTC, parsed from day / pay_date).

user_id, value_prop_id.

position (prints/taps) or amount (payments).

Rejects invalid rows ‚Üí quarantine CSV.

Writes manifest JSON with metrics (rows read, rejected, etc.).

Concrete extractors:

PrintsExtractor ‚Üí JSONL with event_data.

TapsExtractor ‚Üí JSONL with event_data.

PaymentsExtractor ‚Üí CSV with pay_date, total.

5. Validators (validators.py)

Example: taps_have_prints
Ensures each tap has a corresponding print (same user_id, value_prop_id, date).

üß© Data Flow
ETL Flow (Bronze ‚Üí Silver ‚Üí Gold)

``` mermaid
flowchart LR
  subgraph Bronze["Bronze (raw)"]
    P[prints.jsonl] --> E1
    T[taps.jsonl] --> E2
    Y[pays.csv] --> E3
  end

  subgraph Silver["Silver (processed)"]
    E1[PrintsExtractor] --> V1
    E2[TapsExtractor] --> V2
    E3[PaymentsExtractor] --> V3
  end

  subgraph Gold["Gold (final ML dataset)"]
    V1 --> F
    V2 --> F
    V3 --> F
    F[Feature Engine / Aggregator (TBD)]
  end
```

Extraction Workflow

``` mermaid
[Raw file] ‚Üí [Loader (chunk)] ‚Üí [Normalize] ‚Üí [Pydantic row validate]
           ‚Üí [Pandera DF validate] ‚Üí [Processed chunk] ‚Üí [Yield / Write]
```

‚öôÔ∏è Usage (so far)
Configure

Edit config/params.yaml:

``` yaml
processing:
  chunk_size: 10000
  target_window_days: 7
  lookback_window_days: 21
validation:
  business_rules:
    min_timestamp_days_ago: 1825

```
Run Extraction

Example (inside scripts/ or notebook):

``` python
from src.data.extractors import PrintsExtractor
from carrusel_mp.config import get_config

cfg = get_config()
prints_path = cfg.data_paths.raw + "/prints.json"

ext = PrintsExtractor(prints_path)
for chunk in ext.extract_chunked():
    print(chunk.head())

```


